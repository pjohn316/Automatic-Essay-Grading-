# -*- coding: utf-8 -*-
"""20212AIE0002_prompt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PL_ryi37rs8EA6jLwsOXyCS9JC5U7lhi
"""

from pandas.core.algorithms import value_counts_arraylike
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd

df = pd.read_csv("train.tsv", sep = '\t')
#df = pd.read_csv("dev.tsv", sep = '\t')
#df = pd.concat([train, dev]).reset_index()

def count_chars(text):
    return len(text)

def count_words(text):
    return len(text.split())

def count_punctuations(text):
    punctuations="""!"#&(),./:;?@'"""
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d

def punct(d):
  a,b,c,e,f,g,h,j,l,m,n,o,p,q = [],[],[],[],[],[],[],[],[],[],[],[],[],[]
  for i in d:
    for k,v in i.items():
      if k == '! count':
        a.append(i[k])
      elif k == '" count':
        b.append(i[k])
      elif k == '# count':
        c.append(i[k])
      elif k == '& count':
        e.append(i[k])
      elif k == '( count':
        f.append(i[k])
      elif k == ') count':
        g.append(i[k])
      elif k == ', count':
        h.append(i[k])
      elif k == '. count':
        j.append(i[k])
      elif k == '/ count':
        l.append(i[k])
      elif k == ': count':
        m.append(i[k])
      elif k == '; count':
        n.append(i[k])
      elif k == '? count':
        o.append(i[k])
      elif k == '@ count':
        p.append(i[k])
      elif k == "' count":
        q.append(i[k])
  return a,b,c,e,f,g,h,j,l,m,n,o,p,q

def count_sent(text):
    return len(nltk.sent_tokenize(text))

def count_unique_words(text):
    return len(set(text.split()))

def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

def syntactic(text):
  nn,nv,nad,nadv,nfw = 0,0,0,0,0
  taggedSentence = nltk.pos_tag(word_tokenize(text))
  for i in taggedSentence:
    if(i[1] == "NN" or i[1] == "NNS" or i[1] == "NNP" or i[1] == "NNSP"):
      nn = nn + 1
    if(i[1] == "VB" or i[1] == "VBD" or i[1] == "VBG" or i[1] == "VBN" or i[1] == "VBP" or i[1] == "VBZ"):
      nv = nv + 1
    if(i[1] == "RB" or i[1] == "RBR" or i[1] == "RBS"):
      nad = nad + 1
    if(i[1] == "JJ" or i[1] == "JJR" or i[1] == "JJS"):
      nadv = nadv + 1

  return taggedSentence,nn,nv,nad,nadv


c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
for i in range(len(df['essay'])):
  text = df['essay'][i]
  c.append(count_chars(text))
  w.append(count_words(text))
  s.append(count_sent(text))
  u.append(count_unique_words(text))
  sw.append(count_stopwords(text))
  awl.append(count_chars(text)/count_words(text))
  tag,noun,verb,adj,adv = syntactic(text)
  p.append(count_punctuations(text))
  nn.append(noun)
  nv.append(verb)
  nad.append(adj)
  nadv.append(adv)

df['charcount'],df['wordcount'],df['sentcount'],df['uniq_wordcount'],df['stopwordscount'],df['avg_wordlen'] = c,w,s,u,sw,awl
df['! count'],df['" count'],df['# count'],df['& count'],df['( count'],df[') count'],df[', count'],df['. count'],df['/ count'],df[': count'],df['; count'],df['? count'],df['@ count'],df["' count"] = punct(p)
df['nouncount'],df['verbcount'],df['adjectivecount'],df['adverbcount'] = nn,nv,nad,nadv


df.to_csv('feature_extraction_train4.csv')

df.head()

import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error

train = pd.read_csv('feature_extraction_train4.csv')
dev = pd.read_csv('feature_extraction_dev4.csv')

# Classifiers
Xtrain = train.iloc[:,[12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35]].values
Ytrain1 = train['domain1_score']
Ytrain2 = train['trait1_score']
Ytrain3 = train['trait2_score']
Ytrain4 = train['trait3_score']
Ytrain5 = train['trait4_score']
Xtest = dev.iloc[:,[12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35]].values
Ytest1 = dev['domain1_score']
Ytest2 = dev['trait1_score']
Ytest3 = dev['trait2_score']
Ytest4 = dev['trait3_score']
Ytest5 = dev['trait4_score']

# Random Forest
Rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rfc1 = Rf.fit(Xtrain, Ytrain1).predict(Xtest)
rfc2 = Rf.fit(Xtrain, Ytrain2).predict(Xtest)
rfc3 = Rf.fit(Xtrain, Ytrain3).predict(Xtest)
rfc4 = Rf.fit(Xtrain, Ytrain4).predict(Xtest)
rfc5 = Rf.fit(Xtrain, Ytrain5).predict(Xtest)

dev['domain1_score_Rf'],dev['trait1_score_Rf'],dev['trait2_score_Rf'],dev['trait3_score_Rf'],dev['trait4_score_Rf'] = rfc1,rfc2,rfc3,rfc4,rfc5

raY1 = str(accuracy_score(Ytest1, rfc1))
ck1 = cohen_kappa_score(Ytest1, rfc1, weights = 'quadratic')
rrmse1 = mean_squared_error(Ytest1, rfc1)
rrmae1 = mean_absolute_error(Ytest1, rfc1)

raY2 = str(accuracy_score(Ytest2, rfc2))
ck2 = cohen_kappa_score(Ytest2, rfc2, weights = 'quadratic')
rrmse2 = mean_squared_error(Ytest2, rfc2)
rrmae2 = mean_absolute_error(Ytest2, rfc2)

raY3 = str(accuracy_score(Ytest3, rfc3))
ck3 = cohen_kappa_score(Ytest3, rfc3, weights = 'quadratic')
rrmse3 = mean_squared_error(Ytest3, rfc3)
rrmae3 = mean_absolute_error(Ytest3, rfc3)

raY4 = str(accuracy_score(Ytest4, rfc4))
ck4 = cohen_kappa_score(Ytest4, rfc4, weights = 'quadratic')
rrmse4 = mean_squared_error(Ytest4, rfc4)
rrmae4 = mean_absolute_error(Ytest4, rfc4)

raY5 = str(accuracy_score(Ytest5, rfc5))
ck5 = cohen_kappa_score(Ytest5, rfc5, weights = 'quadratic')
rrmse5 = mean_squared_error(Ytest5, rfc5)
rrmae5 = mean_absolute_error(Ytest5, rfc5)

parameters = pd.DataFrame({
    'Parameter': ['Kappa','Accuracy score','Mean Square Error','Mean Absolute Error'],
    'Domain_score': [ck1, raY1, rrmse1, rrmae1],
    'trait1_score': [ck2, raY2, rrmse2, rrmae2],
    'trait2_score': [ck3, raY3, rrmse3, rrmae3],
    'trait3_score': [ck4, raY4, rrmse4, rrmae4],
    'trait4_score': [ck5, raY5, rrmse5, rrmae5]})

print("\nRandon Forest: \n")
print(parameters)


# Logistic regression
LR = LogisticRegression()
lr1 = LR.fit(Xtrain, Ytrain1).predict(Xtest)
lr2 = LR.fit(Xtrain, Ytrain2).predict(Xtest)
lr3 = LR.fit(Xtrain, Ytrain3).predict(Xtest)
lr4 = LR.fit(Xtrain, Ytrain4).predict(Xtest)
lr5 = LR.fit(Xtrain, Ytrain5).predict(Xtest)

dev['domain1_score_lr'],dev['trait1_score_lr'],dev['trait2_score_lr'],dev['trait3_score_lr'],dev['trait4_score_lr'] = lr1,lr2,lr3,lr4,lr5

laY1 = str(accuracy_score(Ytest1, lr1))
lck1 = cohen_kappa_score(Ytest1, lr1, weights = 'quadratic')
lrmse1 = mean_squared_error(Ytest1, lr1)
lrmae1 = mean_absolute_error(Ytest1, lr1)

laY2 = str(accuracy_score(Ytest2, lr2))
lck2 = cohen_kappa_score(Ytest2, lr2, weights = 'quadratic')
lrmse2 = mean_squared_error(Ytest2, lr2)
lrmae2 = mean_absolute_error(Ytest2, lr2)

laY3 = str(accuracy_score(Ytest3, lr3))
lck3 = cohen_kappa_score(Ytest3, lr3, weights = 'quadratic')
lrmse3 = mean_squared_error(Ytest3, lr3)
lrmae3 = mean_absolute_error(Ytest3, lr3)

laY4 = str(accuracy_score(Ytest4, lr4))
lck4 = cohen_kappa_score(Ytest4, lr4, weights = 'quadratic')
lrmse4 = mean_squared_error(Ytest4, lr4)
lrmae4 = mean_absolute_error(Ytest4, lr4)

laY5 = str(accuracy_score(Ytest5, lr5))
lck5 = cohen_kappa_score(Ytest5, lr5, weights = 'quadratic')
lrmse5 = mean_squared_error(Ytest5, lr5)
lrmae5 = mean_absolute_error(Ytest5, lr5)

parameters1 = pd.DataFrame({
    'Parameter': ['Kappa','Accuracy score','Mean Square Error','Mean Absolute Error'],
    'Domain_score': [lck1, laY1, lrmse1, lrmae1],
    'trait1_score': [lck2, laY2, lrmse2, lrmae2],
    'trait2_score': [lck3, laY3, lrmse3, lrmae3],
    'trait3_score': [lck4, laY4, lrmse4, lrmae4],
    'trait4_score': [lck5, laY5, lrmse5, lrmae5]})

print("\nLogistic Regression: \n")
print(parameters1)

dev.to_csv('devfinal4.csv')
parameters.to_csv('Randomforestresults4.csv')
parameters1.to_csv('LogisticRegression4.csv')

from pandas.core.algorithms import value_counts_arraylike
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd



def count_chars(text):
    return len(text)

def count_words(text):
    return len(text.split())

def count_punctuations(text):
    punctuations="""!"#&(),./:;?@'"""
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d

def punct(d):
  a,b,c,e,f,g,h,j,l,m,n,o,p,q = [],[],[],[],[],[],[],[],[],[],[],[],[],[]
  for i in d:
    for k,v in i.items():
      if k == '! count':
        a.append(i[k])
      elif k == '" count':
        b.append(i[k])
      elif k == '# count':
        c.append(i[k])
      elif k == '& count':
        e.append(i[k])
      elif k == '( count':
        f.append(i[k])
      elif k == ') count':
        g.append(i[k])
      elif k == ', count':
        h.append(i[k])
      elif k == '. count':
        j.append(i[k])
      elif k == '/ count':
        l.append(i[k])
      elif k == ': count':
        m.append(i[k])
      elif k == '; count':
        n.append(i[k])
      elif k == '? count':
        o.append(i[k])
      elif k == '@ count':
        p.append(i[k])
      elif k == "' count":
        q.append(i[k])
  return a,b,c,e,f,g,h,j,l,m,n,o,p,q

def count_sent(text):
    return len(nltk.sent_tokenize(text))

def count_unique_words(text):
    return len(set(text.split()))

def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

def syntactic(text):
  nn,nv,nad,nadv,nfw = 0,0,0,0,0
  taggedSentence = nltk.pos_tag(word_tokenize(text))
  for i in taggedSentence:
    if(i[1] == "NN" or i[1] == "NNS" or i[1] == "NNP" or i[1] == "NNSP"):
      nn = nn + 1
    if(i[1] == "VB" or i[1] == "VBD" or i[1] == "VBG" or i[1] == "VBN" or i[1] == "VBP" or i[1] == "VBZ"):
      nv = nv + 1
    if(i[1] == "RB" or i[1] == "RBR" or i[1] == "RBS"):
      nad = nad + 1
    if(i[1] == "JJ" or i[1] == "JJR" or i[1] == "JJS"):
      nadv = nadv + 1

  return taggedSentence,nn,nv,nad,nadv



c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
with open('test0.txt') as f:
    text = f.readlines()
    #text = text.replace("\n"," ")
    c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
    for i in text:
      c.append(count_chars(i))
      w.append(count_words(i))
      s.append(count_sent(i))
      u.append(count_unique_words(i))
      sw.append(count_stopwords(i))
      awl.append(count_chars(i)/count_words(i))
      tag,noun,verb,adj,adv = syntactic(i)
      p.append(count_punctuations(i))
      nn.append(noun)
      nv.append(verb)
      nad.append(adj)
      nadv.append(adv)

print(text)
print(c)
p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14 = punct(p)
df = pd.DataFrame(list(zip(c,w,s,u,sw,awl,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,nn,nv,nad,nadv)),
               columns =['charcount', 'wordcount','sentcount','uniq_wordcount','stopwordscount','avg_wordlen','! count','" count','# count','& count','( count',') count',', count','. count','/ count',': count','; count','? count','@ count',"' count",'nouncount','verbcount','adjectivecount','adverbcount'])

df.to_csv('feature_extraction_test0.csv')

df.head()

from pandas.core.algorithms import value_counts_arraylike
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd



def count_chars(text):
    return len(text)

def count_words(text):
    return len(text.split())

def count_punctuations(text):
    punctuations="""!"#&(),./:;?@'"""
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d

def punct(d):
  a,b,c,e,f,g,h,j,l,m,n,o,p,q = [],[],[],[],[],[],[],[],[],[],[],[],[],[]
  for i in d:
    for k,v in i.items():
      if k == '! count':
        a.append(i[k])
      elif k == '" count':
        b.append(i[k])
      elif k == '# count':
        c.append(i[k])
      elif k == '& count':
        e.append(i[k])
      elif k == '( count':
        f.append(i[k])
      elif k == ') count':
        g.append(i[k])
      elif k == ', count':
        h.append(i[k])
      elif k == '. count':
        j.append(i[k])
      elif k == '/ count':
        l.append(i[k])
      elif k == ': count':
        m.append(i[k])
      elif k == '; count':
        n.append(i[k])
      elif k == '? count':
        o.append(i[k])
      elif k == '@ count':
        p.append(i[k])
      elif k == "' count":
        q.append(i[k])
  return a,b,c,e,f,g,h,j,l,m,n,o,p,q

def count_sent(text):
    return len(nltk.sent_tokenize(text))

def count_unique_words(text):
    return len(set(text.split()))

def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

def syntactic(text):
  nn,nv,nad,nadv,nfw = 0,0,0,0,0
  taggedSentence = nltk.pos_tag(word_tokenize(text))
  for i in taggedSentence:
    if(i[1] == "NN" or i[1] == "NNS" or i[1] == "NNP" or i[1] == "NNSP"):
      nn = nn + 1
    if(i[1] == "VB" or i[1] == "VBD" or i[1] == "VBG" or i[1] == "VBN" or i[1] == "VBP" or i[1] == "VBZ"):
      nv = nv + 1
    if(i[1] == "RB" or i[1] == "RBR" or i[1] == "RBS"):
      nad = nad + 1
    if(i[1] == "JJ" or i[1] == "JJR" or i[1] == "JJS"):
      nadv = nadv + 1

  return taggedSentence,nn,nv,nad,nadv



c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
with open('test1.txt') as f:
    text = f.readlines()
    #text = text.replace("\n"," ")
    c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
    for i in text:
      c.append(count_chars(i))
      w.append(count_words(i))
      s.append(count_sent(i))
      u.append(count_unique_words(i))
      sw.append(count_stopwords(i))
      awl.append(count_chars(i)/count_words(i))
      tag,noun,verb,adj,adv = syntactic(i)
      p.append(count_punctuations(i))
      nn.append(noun)
      nv.append(verb)
      nad.append(adj)
      nadv.append(adv)

print(text)
print(c)
p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14 = punct(p)
df = pd.DataFrame(list(zip(c,w,s,u,sw,awl,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,nn,nv,nad,nadv)),
               columns =['charcount', 'wordcount','sentcount','uniq_wordcount','stopwordscount','avg_wordlen','! count','" count','# count','& count','( count',') count',', count','. count','/ count',': count','; count','? count','@ count',"' count",'nouncount','verbcount','adjectivecount','adverbcount'])

df.to_csv('feature_extraction_test1.csv')

df.head()

from pandas.core.algorithms import value_counts_arraylike
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd



def count_chars(text):
    return len(text)

def count_words(text):
    return len(text.split())

def count_punctuations(text):
    punctuations="""!"#&(),./:;?@'"""
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d

def punct(d):
  a,b,c,e,f,g,h,j,l,m,n,o,p,q = [],[],[],[],[],[],[],[],[],[],[],[],[],[]
  for i in d:
    for k,v in i.items():
      if k == '! count':
        a.append(i[k])
      elif k == '" count':
        b.append(i[k])
      elif k == '# count':
        c.append(i[k])
      elif k == '& count':
        e.append(i[k])
      elif k == '( count':
        f.append(i[k])
      elif k == ') count':
        g.append(i[k])
      elif k == ', count':
        h.append(i[k])
      elif k == '. count':
        j.append(i[k])
      elif k == '/ count':
        l.append(i[k])
      elif k == ': count':
        m.append(i[k])
      elif k == '; count':
        n.append(i[k])
      elif k == '? count':
        o.append(i[k])
      elif k == '@ count':
        p.append(i[k])
      elif k == "' count":
        q.append(i[k])
  return a,b,c,e,f,g,h,j,l,m,n,o,p,q

def count_sent(text):
    return len(nltk.sent_tokenize(text))

def count_unique_words(text):
    return len(set(text.split()))

def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

def syntactic(text):
  nn,nv,nad,nadv,nfw = 0,0,0,0,0
  taggedSentence = nltk.pos_tag(word_tokenize(text))
  for i in taggedSentence:
    if(i[1] == "NN" or i[1] == "NNS" or i[1] == "NNP" or i[1] == "NNSP"):
      nn = nn + 1
    if(i[1] == "VB" or i[1] == "VBD" or i[1] == "VBG" or i[1] == "VBN" or i[1] == "VBP" or i[1] == "VBZ"):
      nv = nv + 1
    if(i[1] == "RB" or i[1] == "RBR" or i[1] == "RBS"):
      nad = nad + 1
    if(i[1] == "JJ" or i[1] == "JJR" or i[1] == "JJS"):
      nadv = nadv + 1

  return taggedSentence,nn,nv,nad,nadv



c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
with open('test2.txt') as f:
    text = f.readlines()
    #text = text.replace("\n"," ")
    c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
    for i in text:
      c.append(count_chars(i))
      w.append(count_words(i))
      s.append(count_sent(i))
      u.append(count_unique_words(i))
      sw.append(count_stopwords(i))
      awl.append(count_chars(i)/count_words(i))
      tag,noun,verb,adj,adv = syntactic(i)
      p.append(count_punctuations(i))
      nn.append(noun)
      nv.append(verb)
      nad.append(adj)
      nadv.append(adv)

print(text)
print(c)
p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14 = punct(p)
df = pd.DataFrame(list(zip(c,w,s,u,sw,awl,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,nn,nv,nad,nadv)),
               columns =['charcount', 'wordcount','sentcount','uniq_wordcount','stopwordscount','avg_wordlen','! count','" count','# count','& count','( count',') count',', count','. count','/ count',': count','; count','? count','@ count',"' count",'nouncount','verbcount','adjectivecount','adverbcount'])

df.to_csv('feature_extraction_test2.csv')

df.head()

from pandas.core.algorithms import value_counts_arraylike
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd



def count_chars(text):
    return len(text)

def count_words(text):
    return len(text.split())

def count_punctuations(text):
    punctuations="""!"#&(),./:;?@'"""
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d

def punct(d):
  a,b,c,e,f,g,h,j,l,m,n,o,p,q = [],[],[],[],[],[],[],[],[],[],[],[],[],[]
  for i in d:
    for k,v in i.items():
      if k == '! count':
        a.append(i[k])
      elif k == '" count':
        b.append(i[k])
      elif k == '# count':
        c.append(i[k])
      elif k == '& count':
        e.append(i[k])
      elif k == '( count':
        f.append(i[k])
      elif k == ') count':
        g.append(i[k])
      elif k == ', count':
        h.append(i[k])
      elif k == '. count':
        j.append(i[k])
      elif k == '/ count':
        l.append(i[k])
      elif k == ': count':
        m.append(i[k])
      elif k == '; count':
        n.append(i[k])
      elif k == '? count':
        o.append(i[k])
      elif k == '@ count':
        p.append(i[k])
      elif k == "' count":
        q.append(i[k])
  return a,b,c,e,f,g,h,j,l,m,n,o,p,q

def count_sent(text):
    return len(nltk.sent_tokenize(text))

def count_unique_words(text):
    return len(set(text.split()))

def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

def syntactic(text):
  nn,nv,nad,nadv,nfw = 0,0,0,0,0
  taggedSentence = nltk.pos_tag(word_tokenize(text))
  for i in taggedSentence:
    if(i[1] == "NN" or i[1] == "NNS" or i[1] == "NNP" or i[1] == "NNSP"):
      nn = nn + 1
    if(i[1] == "VB" or i[1] == "VBD" or i[1] == "VBG" or i[1] == "VBN" or i[1] == "VBP" or i[1] == "VBZ"):
      nv = nv + 1
    if(i[1] == "RB" or i[1] == "RBR" or i[1] == "RBS"):
      nad = nad + 1
    if(i[1] == "JJ" or i[1] == "JJR" or i[1] == "JJS"):
      nadv = nadv + 1

  return taggedSentence,nn,nv,nad,nadv



c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
with open('test3.txt') as f:
    text = f.readlines()
    #text = text.replace("\n"," ")
    c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
    for i in text:
      c.append(count_chars(i))
      w.append(count_words(i))
      s.append(count_sent(i))
      u.append(count_unique_words(i))
      sw.append(count_stopwords(i))
      awl.append(count_chars(i)/count_words(i))
      tag,noun,verb,adj,adv = syntactic(i)
      p.append(count_punctuations(i))
      nn.append(noun)
      nv.append(verb)
      nad.append(adj)
      nadv.append(adv)

print(text)
print(c)
p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14 = punct(p)
df = pd.DataFrame(list(zip(c,w,s,u,sw,awl,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,nn,nv,nad,nadv)),
               columns =['charcount', 'wordcount','sentcount','uniq_wordcount','stopwordscount','avg_wordlen','! count','" count','# count','& count','( count',') count',', count','. count','/ count',': count','; count','? count','@ count',"' count",'nouncount','verbcount','adjectivecount','adverbcount'])

df.to_csv('feature_extraction_test3.csv')

df.head()

from pandas.core.algorithms import value_counts_arraylike
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd



def count_chars(text):
    return len(text)

def count_words(text):
    return len(text.split())

def count_punctuations(text):
    punctuations="""!"#&(),./:;?@'"""
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d

def punct(d):
  a,b,c,e,f,g,h,j,l,m,n,o,p,q = [],[],[],[],[],[],[],[],[],[],[],[],[],[]
  for i in d:
    for k,v in i.items():
      if k == '! count':
        a.append(i[k])
      elif k == '" count':
        b.append(i[k])
      elif k == '# count':
        c.append(i[k])
      elif k == '& count':
        e.append(i[k])
      elif k == '( count':
        f.append(i[k])
      elif k == ') count':
        g.append(i[k])
      elif k == ', count':
        h.append(i[k])
      elif k == '. count':
        j.append(i[k])
      elif k == '/ count':
        l.append(i[k])
      elif k == ': count':
        m.append(i[k])
      elif k == '; count':
        n.append(i[k])
      elif k == '? count':
        o.append(i[k])
      elif k == '@ count':
        p.append(i[k])
      elif k == "' count":
        q.append(i[k])
  return a,b,c,e,f,g,h,j,l,m,n,o,p,q

def count_sent(text):
    return len(nltk.sent_tokenize(text))

def count_unique_words(text):
    return len(set(text.split()))

def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

def syntactic(text):
  nn,nv,nad,nadv,nfw = 0,0,0,0,0
  taggedSentence = nltk.pos_tag(word_tokenize(text))
  for i in taggedSentence:
    if(i[1] == "NN" or i[1] == "NNS" or i[1] == "NNP" or i[1] == "NNSP"):
      nn = nn + 1
    if(i[1] == "VB" or i[1] == "VBD" or i[1] == "VBG" or i[1] == "VBN" or i[1] == "VBP" or i[1] == "VBZ"):
      nv = nv + 1
    if(i[1] == "RB" or i[1] == "RBR" or i[1] == "RBS"):
      nad = nad + 1
    if(i[1] == "JJ" or i[1] == "JJR" or i[1] == "JJS"):
      nadv = nadv + 1

  return taggedSentence,nn,nv,nad,nadv



c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
with open('test4.txt') as f:
    text = f.readlines()
    #text = text.replace("\n"," ")
    c,w,p,pc,s,u,sw,awl,nn,nv,nad,nadv = [],[],[],[],[],[],[],[],[],[],[],[]
    for i in text:
      c.append(count_chars(i))
      w.append(count_words(i))
      s.append(count_sent(i))
      u.append(count_unique_words(i))
      sw.append(count_stopwords(i))
      awl.append(count_chars(i)/count_words(i))
      tag,noun,verb,adj,adv = syntactic(i)
      p.append(count_punctuations(i))
      nn.append(noun)
      nv.append(verb)
      nad.append(adj)
      nadv.append(adv)

print(text)
print(c)
p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14 = punct(p)
df = pd.DataFrame(list(zip(c,w,s,u,sw,awl,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,nn,nv,nad,nadv)),
               columns =['charcount', 'wordcount','sentcount','uniq_wordcount','stopwordscount','avg_wordlen','! count','" count','# count','& count','( count',') count',', count','. count','/ count',': count','; count','? count','@ count',"' count",'nouncount','verbcount','adjectivecount','adverbcount'])

df.to_csv('feature_extraction_test4.csv')

df.head()

import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error

train = pd.read_csv('feature_extraction_train0.csv')
test = pd.read_csv('feature_extraction_test0.csv')

# Classifiers
Xtrain = train.iloc[:,[13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]].values
Ytrain1 = train['domain1_score']
Ytrain2 = train['trait1_score']
Ytrain3 = train['trait2_score']
Ytrain4 = train['trait3_score']
Ytrain5 = train['trait4_score']
Ytrain6 = train['trait5_score']
Xtest = test.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]].values


# Random Forest
Rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rfc1 = Rf.fit(Xtrain, Ytrain1).predict(Xtest)
rfc2 = Rf.fit(Xtrain, Ytrain2).predict(Xtest)
rfc3 = Rf.fit(Xtrain, Ytrain3).predict(Xtest)
rfc4 = Rf.fit(Xtrain, Ytrain4).predict(Xtest)
rfc5 = Rf.fit(Xtrain, Ytrain5).predict(Xtest)
rfc6 = Rf.fit(Xtrain, Ytrain6).predict(Xtest)
test['domain1_score_Rf'],test['trait1_score_Rf'],test['trait2_score_Rf'],test['trait3_score_Rf'],test['trait4_score_Rf'],test['trait5_score_Rf'] = rfc1,rfc2,rfc3,rfc4,rfc5,rfc6





test.to_csv('testfinal0.csv')

import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error

train = pd.read_csv('feature_extraction_train1.csv')
test = pd.read_csv('feature_extraction_test1.csv')

# Classifiers
Xtrain = train.iloc[:,[13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]].values
Ytrain1 = train['domain1_score']
Ytrain2 = train['trait1_score']
Ytrain3 = train['trait2_score']
Ytrain4 = train['trait3_score']
Ytrain5 = train['trait4_score']
Ytrain6 = train['trait5_score']
Xtest = test.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]].values


# Random Forest
Rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rfc1 = Rf.fit(Xtrain, Ytrain1).predict(Xtest)
rfc2 = Rf.fit(Xtrain, Ytrain2).predict(Xtest)
rfc3 = Rf.fit(Xtrain, Ytrain3).predict(Xtest)
rfc4 = Rf.fit(Xtrain, Ytrain4).predict(Xtest)
rfc5 = Rf.fit(Xtrain, Ytrain5).predict(Xtest)
rfc6 = Rf.fit(Xtrain, Ytrain6).predict(Xtest)
test['domain1_score_Rf'],test['trait1_score_Rf'],test['trait2_score_Rf'],test['trait3_score_Rf'],test['trait4_score_Rf'],test['trait5_score_Rf'] = rfc1,rfc2,rfc3,rfc4,rfc5,rfc6





test.to_csv('testfinal1.csv')

import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error

train = pd.read_csv('feature_extraction_train2.csv')
test = pd.read_csv('feature_extraction_test2.csv')

# Classifiers
Xtrain = train.iloc[:,[13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]].values
Ytrain1 = train['domain1_score']
Ytrain2 = train['trait1_score']
Ytrain3 = train['trait2_score']
Ytrain4 = train['trait3_score']
Ytrain5 = train['trait4_score']
Ytrain6 = train['trait5_score']
Xtest = test.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]].values


# Random Forest
Rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rfc1 = Rf.fit(Xtrain, Ytrain1).predict(Xtest)
rfc2 = Rf.fit(Xtrain, Ytrain2).predict(Xtest)
rfc3 = Rf.fit(Xtrain, Ytrain3).predict(Xtest)
rfc4 = Rf.fit(Xtrain, Ytrain4).predict(Xtest)
rfc5 = Rf.fit(Xtrain, Ytrain5).predict(Xtest)
rfc6 = Rf.fit(Xtrain, Ytrain6).predict(Xtest)
test['domain1_score_Rf'],test['trait1_score_Rf'],test['trait2_score_Rf'],test['trait3_score_Rf'],test['trait4_score_Rf'],test['trait5_score_Rf'] = rfc1,rfc2,rfc3,rfc4,rfc5,rfc6





test.to_csv('testfinal2.csv')

import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error

train = pd.read_csv('feature_extraction_train3.csv')
test = pd.read_csv('feature_extraction_test3.csv')

# Classifiers
Xtrain = train.iloc[:,[13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]].values
Ytrain1 = train['domain1_score']
Ytrain2 = train['trait1_score']
Ytrain3 = train['trait2_score']
Ytrain4 = train['trait3_score']
Ytrain5 = train['trait4_score']
Ytrain6 = train['trait5_score']
Xtest = test.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]].values


# Random Forest
Rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rfc1 = Rf.fit(Xtrain, Ytrain1).predict(Xtest)
rfc2 = Rf.fit(Xtrain, Ytrain2).predict(Xtest)
rfc3 = Rf.fit(Xtrain, Ytrain3).predict(Xtest)
rfc4 = Rf.fit(Xtrain, Ytrain4).predict(Xtest)
rfc5 = Rf.fit(Xtrain, Ytrain5).predict(Xtest)
rfc6 = Rf.fit(Xtrain, Ytrain6).predict(Xtest)
test['domain1_score_Rf'],test['trait1_score_Rf'],test['trait2_score_Rf'],test['trait3_score_Rf'],test['trait4_score_Rf'],test['trait5_score_Rf'] = rfc1,rfc2,rfc3,rfc4,rfc5,rfc6





test.to_csv('testfinal3.csv')

import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk.data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import pandas as pd
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error

train = pd.read_csv('feature_extraction_train4.csv')
test = pd.read_csv('feature_extraction_test4.csv')

# Classifiers
Xtrain = train.iloc[:,[13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]].values
Ytrain1 = train['domain1_score']
Ytrain2 = train['trait1_score']
Ytrain3 = train['trait2_score']
Ytrain4 = train['trait3_score']
Ytrain5 = train['trait4_score']
Ytrain6 = train['trait5_score']
Xtest = test.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]].values


# Random Forest
Rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rfc1 = Rf.fit(Xtrain, Ytrain1).predict(Xtest)
rfc2 = Rf.fit(Xtrain, Ytrain2).predict(Xtest)
rfc3 = Rf.fit(Xtrain, Ytrain3).predict(Xtest)
rfc4 = Rf.fit(Xtrain, Ytrain4).predict(Xtest)
rfc5 = Rf.fit(Xtrain, Ytrain5).predict(Xtest)
rfc6 = Rf.fit(Xtrain, Ytrain6).predict(Xtest)
test['domain1_score_Rf'],test['trait1_score_Rf'],test['trait2_score_Rf'],test['trait3_score_Rf'],test['trait4_score_Rf'],test['trait5_score_Rf'] = rfc1,rfc2,rfc3,rfc4,rfc5,rfc6





test.to_csv('testfinal4.csv')